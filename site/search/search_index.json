{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AutoDask","text":"<p>Note: AutoDask is currently in the development stage, which is why some modules may not work or may have errors.</p>"},{"location":"#the-distributed-automl-solution","title":"The Distributed AutoML Solution","text":"<p>AutoDask is a lightweight AutoML library that brings together the power of distributed computing with Dask and the intelligence of Bee Colony Optimization for hyperparameter tuning.</p> <pre><code>pip install autodask\n</code></pre> <pre><code>from autodask.main import AutoDask\n\n# Create an AutoDask instance\nadsk = AutoDask(task='classification')\n\n# Train the model\nadsk.fit(X_train, y_train)\n\n# Make predictions\npredictions = adsk.predict(X_test)\n</code></pre>"},{"location":"#why-choose-autodask","title":"Why Choose AutoDask?","text":""},{"location":"#distributed-performance","title":"\ud83d\ude80 Distributed Performance","text":"<p>Built on Dask to scale from your laptop to your cluster with minimal configuration changes.</p>"},{"location":"#bio-inspired-optimization","title":"\ud83d\udc1d Bio-Inspired Optimization","text":"<p>Uses Bee Colony Optimization to intelligently navigate the vast hyperparameter space with fewer iterations than traditional methods.</p>"},{"location":"#smart-resource-management","title":"\ud83e\udde0 Smart Resource Management","text":"<p>Automatically allocates computational resources for maximum efficiency during model training and evaluation.</p>"},{"location":"#end-to-end-pipeline","title":"\ud83d\udd04 End-to-End Pipeline","text":"<p>Handles everything from preprocessing and feature engineering to model selection and ensemble creation.</p>"},{"location":"#core-features","title":"Core Features","text":"<ul> <li>Multi-Task Support: Classification and regression workflows</li> <li>Distributed Computing: Parallel model training and evaluation</li> <li>Automated Feature Engineering: Intelligent preprocessing and transformation</li> <li>Hyperparameter Optimization: Nature-inspired BCO algorithm</li> <li>Model Ensembling: Combines top-performing models for improved accuracy</li> <li>Time Management: Set time constraints for your AutoML workflows</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Basic Tutorial</li> <li>API Reference</li> <li>Bee Colony Optimization Details</li> </ul>"},{"location":"#example-use-cases","title":"Example Use Cases","text":"<p>Coming soon...</p>"},{"location":"autodask/advanced_usage/","title":"Using Custom BCO Parameters","text":"<pre><code>from autodask.main import AutoDask\n\n# Configure with custom BCO parameters\nadsk = AutoDask(\n    task='classification',\n    with_tuning=True,\n    bco_params={\n        'employed_bees': 20,     # Number of employed bees\n        'onlooker_bees': 10,     # Number of onlooker bees\n        'scout_bees': 5,         # Number of scout bees\n        'abandonment_limit': 10, # Limit before abandoning a solution\n        'exploration_rate': 0.3  # Balance between exploration and exploitation\n    }\n)\n\n# Train the model\nadsk.fit(X_train, y_train)\n</code></pre>"},{"location":"autodask/basic_usage/","title":"Basic Classification","text":"<pre><code>from autodask.main import AutoDask\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\n\n# Load sample data\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = iris.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train AutoDask instance\nadsk = AutoDask(task='classification')\nadsk.fit(X_train, y_train)\n\n# Make predictions\npredictions = adsk.predict(X_test)\n</code></pre>"},{"location":"autodask/basic_usage/#saving-a-model","title":"Saving a Model","text":"<pre><code>from autodask.main import AutoDask\n\n# Create a new instance\nadsk = AutoDask(task='classification')\n\n# Load a saved model\nadsk.save('adsk_model.pkl')\n</code></pre>"},{"location":"autodask/basic_usage/#loading-a-saved-model","title":"Loading a Saved Model","text":"<pre><code>from autodask.main import AutoDask\n\n# Create a new instance\nadsk = AutoDask(task='classification')\n\n# Load a saved model\nadsk.load_model('adsk_model.pkl')\n\n# Use the loaded model to make predictions\npredictions = adsk.predict(X_new)\n</code></pre>"},{"location":"autodask/main/","title":"Orchestrating class for AutoDask","text":""},{"location":"autodask/main/#class-autodask","title":"Class: AutoDask","text":"<pre><code>from autodask.main import AutoDask\n</code></pre>"},{"location":"autodask/main/#description","title":"Description","text":"<p>An end-to-end automated machine learning solution that handles preprocessing, model selection, hyperparameter tuning, and ensemble construction. It supports both classification and regression tasks with parallel execution using Dask.</p>"},{"location":"autodask/main/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>task</code> str required The machine learning task type. Supported values: 'classification', 'regression'. <code>n_jobs</code> int 4 Number of parallel jobs to run. Set to -1 to use all available cores. <code>with_tuning</code> bool False Whether to perform hyperparameter tuning using the Bee Colony Optimization algorithm. <code>time_limit</code> int 300 Maximum time in seconds for the AutoML process (default: 5 minutes). <code>metric</code> str None Evaluation metric to optimize. If None, defaults to a task-appropriate metric (accuracy for classification, MSE for regression). <code>cv_folds</code> int 5 Number of cross-validation folds. <code>seed</code> int 101 Random seed for reproducibility. <code>optimization_rounds</code> int 30 Number of optimization rounds for hyperparameter tuning. <code>max_ensemble_models</code> int 3 Maximum number of models in the final ensemble. <code>preprocess</code> bool True Whether to apply automatic preprocessing to the input data. <code>models</code> list None Custom list of models to consider. If None, uses the default model library. <code>bco_params</code> dict {} Parameters for bee colony optimization."},{"location":"autodask/main/#attributes","title":"Attributes","text":"Attribute Type Description <code>ensemble</code> WeightedAverageBlender The final ensemble model after fitting. <code>n_classes</code> int Number of classes (for classification tasks). <code>preprocessor</code> Preprocessor Fitted preprocessing pipeline. <code>log</code> Logger Logger instance for tracking progress."},{"location":"autodask/main/#methods","title":"Methods","text":""},{"location":"autodask/main/#fitx_train-y_train-validation_datanone","title":"<code>fit(X_train, y_train, validation_data=None)</code>","text":"<p>Train the AutoDask model on the given training data.</p> <p>Parameters:</p> Parameter Type Default Description <code>X_train</code> Union[pd.DataFrame, np.ndarray, tuple, dict, list] required Training features. Can be a pandas DataFrame, numpy array, or a tuple/dict/list of arrays (for multi-input models). <code>y_train</code> Union[pd.DataFrame, np.ndarray, tuple, dict, list, str] required Training target. Can be a pandas DataFrame/Series, numpy array, or column name (str) if X_train is a DataFrame. <code>validation_data</code> tuple None Optional tuple (X_val, y_val) for validation. If provided, hold-out validation is used; otherwise, k-fold cross-validation is used. <p>Returns:</p> <p>The instance itself (self), allowing for method chaining.</p> <p>Example:</p> <pre><code># Without validation data (uses k-fold cross-validation)\nadsk = AutoDask(task='classification')\nadsk.fit(X_train, y_train)\n\n# With validation data (uses hold-out validation)\nadsk = AutoDask(task='regression')\nadsk.fit(X_train, y_train, validation_data=(X_val, y_val))\n</code></pre>"},{"location":"autodask/main/#predictx_test","title":"<code>predict(X_test)</code>","text":"<p>Make predictions on new data using the trained ensemble.</p> <p>Parameters:</p> Parameter Type Description <code>X_test</code> Union[pd.DataFrame, np.ndarray, tuple, dict, list] Input features to predict on. Should have the same format as X_train in fit(). <p>Returns:</p> <p>Array of predictions. For classification, returns class labels. For regression, returns continuous values.</p> <p>Example:</p> <pre><code>predictions = adsk.predict(X_test)\n</code></pre>"},{"location":"autodask/main/#predict_probax_test","title":"<code>predict_proba(X_test)</code>","text":"<p>For classification tasks, returns the class probabilities for each sample.</p> <p>Parameters:</p> Parameter Type Description <code>X_test</code> Union[pd.DataFrame, np.ndarray, tuple, dict, list] Input features to predict on. Should have the same format as X_train in fit(). <p>Returns:</p> <p>Array of prediction probabilities for each class.</p> <p>Example:</p> <pre><code>probabilities = adsk.predict_proba(X_test)\n</code></pre>"},{"location":"autodask/main/#get_fitted_ensemble","title":"<code>get_fitted_ensemble()</code>","text":"<p>Get the trained ensemble model.</p> <p>Returns:</p> <p>The fitted <code>WeightedAverageBlender</code> ensemble model.</p> <p>Example:</p> <pre><code>ensemble = adsk.get_fitted_ensemble()\n</code></pre>"},{"location":"autodask/main/#get_fitted_preprocessor","title":"<code>get_fitted_preprocessor()</code>","text":"<p>Get the fitted preprocessing pipeline.</p> <p>Returns:</p> <p>The fitted <code>Preprocessor</code> preprocessing pipeline.</p> <p>Example:</p> <pre><code>preprocessor = adsk.get_fitted_preprocessor()\n</code></pre>"},{"location":"autodask/main/#savepath","title":"<code>save(path)</code>","text":"<p>Save the trained ensemble to disk.</p> <p>Parameters:</p> Parameter Type Description <code>path</code> str File path to save the model to. <p>Example:</p> <pre><code>adsk.save('adsk_model.pkl')\n</code></pre>"},{"location":"autodask/main/#load_modelpath","title":"<code>load_model(path)</code>","text":"<p>Load a previously saved ensemble from disk.</p> <p>Parameters:</p> Parameter Type Description <code>path</code> str File path to load the model from. <p>Example:</p> <pre><code>adsk.load_model('adsk_model.pkl')\nadsk.predict(X_new)  # Can now make predictions\n</code></pre>"},{"location":"autodask/main/#private-methods","title":"Private Methods","text":"<p>These methods are used internally by the AutoDask class:</p> <ul> <li><code>_create_dask_server()</code>: Initializes the Dask distributed computing environment.</li> <li><code>_shutdown_dask_server()</code>: Closes the Dask client and cluster.</li> <li><code>_check_input_correctness(X, y)</code>: Validates and formats the input data.</li> <li><code>_kind_clf(y_train)</code>: Determines the classification type (binary or multiclass) and sets <code>n_classes</code>.</li> </ul>"},{"location":"autodask/main/#notes","title":"Notes","text":"<ul> <li>The <code>AutoDask</code> class automatically handles distributed computing setup using Dask, with a local dashboard available at http://localhost:8787/status during training.</li> <li>When <code>preprocess=True</code>, input data is automatically preprocessed, handling categorical features, missing values, and scaling.</li> <li>For classification tasks, the class automatically determines if it's binary or multiclass.</li> <li>The <code>time_limit</code> parameter ensures the AutoML process completes within the specified time constraint.</li> <li>The Bee Colony Optimization algorithm is used for hyperparameter tuning when <code>with_tuning=True</code>.</li> </ul>"},{"location":"autodask/repository/","title":"ML Models and Metrics Repository","text":""},{"location":"autodask/repository/#overview","title":"Overview","text":"<p>AutoDask provides standardized access to machine learning models and evaluation metrics with the following key components:</p> <ol> <li>AtomizedModel class - Repository of machine learning models with default hyperparameters and hyperparameter search spaces</li> <li>Metrics Repository - Collection of standardized evaluation metrics for classification and regression tasks</li> </ol>"},{"location":"autodask/repository/#atomizedmodel-class","title":"AtomizedModel Class","text":"<p>The <code>AtomizedModel</code> class serves as a repository of machine learning models, providing consistent interfaces for accessing various classification and regression algorithms along with their hyperparameter configurations.</p>"},{"location":"autodask/repository/#key-features","title":"Key Features","text":"<ul> <li>Access to standardized machine learning models</li> <li>Default hyperparameters for quick experimentation</li> <li>Hyperparameter search spaces for optimization</li> </ul>"},{"location":"autodask/repository/#available-models","title":"Available Models","text":""},{"location":"autodask/repository/#classification-models","title":"Classification Models","text":"Model Name Implementation Description <code>l2_logreg</code> <code>LogisticRegression</code> Logistic regression with L2 regularization <code>extra_trees</code> <code>ExtraTreesClassifier</code> Ensemble of extremely randomized trees <code>lgbm</code> <code>LGBMClassifier</code> LightGBM gradient boosting classifier <code>xgboost</code> <code>XGBClassifier</code> XGBoost gradient boosting classifier <code>catboost</code> <code>CatBoostClassifier</code> CatBoost gradient boosting classifier"},{"location":"autodask/repository/#regression-models","title":"Regression Models","text":"Model Name Implementation Description <code>l2_linreg</code> <code>LinearRegression</code> Linear regression with L2 regularization <code>extra_trees</code> <code>ExtraTreesRegressor</code> Ensemble of extremely randomized trees <code>lgbm</code> <code>LGBMRegressor</code> LightGBM gradient boosting regressor <code>xgboost</code> <code>XGBRegressor</code> XGBoost gradient boosting regressor <code>catboost</code> <code>CatBoostRegressor</code> CatBoost gradient boosting regressor"},{"location":"autodask/repository/#methods","title":"Methods","text":""},{"location":"autodask/repository/#get_classifier_models","title":"<code>get_classifier_models()</code>","text":"<p>Returns a dictionary of classification models with their hyperparameter spaces and default configurations.</p> <p>Returns: - Dictionary with model names as keys and tuples of (model class, hyperparameter search space, default hyperparameters) as values</p> <p>Example:</p> <pre><code>models = AtomizedModel.get_classifier_models()\nclassifier = models['lgbm'][0](**models['lgbm'][2])  # Create LGBM with default params\n</code></pre>"},{"location":"autodask/repository/#get_regressor_models","title":"<code>get_regressor_models()</code>","text":"<p>Returns a dictionary of regression models with their hyperparameter spaces and default configurations.</p> <p>Returns: - Dictionary with model names as keys and tuples of (model class, hyperparameter search space, default hyperparameters) as values</p> <p>Example:</p> <pre><code>models = AtomizedModel.get_regressor_models()\nregressor = models['xgboost'][0](**models['xgboost'][2])  # Create XGBoost with default params\n</code></pre>"},{"location":"autodask/repository/#_load_parameter_spaces-private","title":"<code>_load_parameter_spaces()</code> (private)","text":"<p>Loads hyperparameter search spaces from a JSON configuration file.</p> <p>Returns: - Dictionary of parameter search spaces organized by model name</p>"},{"location":"autodask/repository/#_load_parameter_default-private","title":"<code>_load_parameter_default()</code> (private)","text":"<p>Loads default hyperparameters from a JSON configuration file.</p> <p>Returns: - Dictionary of default parameters organized by model name</p>"},{"location":"autodask/repository/#metrics-repository","title":"Metrics Repository","text":"<p>The metrics repository provides a standardized way to access evaluation metrics for machine learning models.</p>"},{"location":"autodask/repository/#available-metrics","title":"Available Metrics","text":""},{"location":"autodask/repository/#classification-metrics","title":"Classification Metrics","text":"Metric Name Function Default Maximize <code>accuracy</code> <code>metrics.accuracy_score</code> Yes Yes <code>f1</code> <code>metrics.f1_score</code> No Yes <code>precision</code> <code>metrics.precision_score</code> No Yes <code>recall</code> <code>metrics.recall_score</code> No Yes"},{"location":"autodask/repository/#regression-metrics","title":"Regression Metrics","text":"Metric Name Function Default Maximize <code>mse</code> <code>metrics.mean_squared_error</code> Yes No <code>r2</code> <code>metrics.r2_score</code> No Yes <code>mae</code> <code>metrics.mean_absolute_error</code> No No <code>rmse</code> Custom (sqrt of MSE) No No"},{"location":"autodask/repository/#functions","title":"Functions","text":""},{"location":"autodask/repository/#get_metricmetric_name-tasknone","title":"<code>get_metric(metric_name, task=None)</code>","text":"<p>Retrieves a specific metric function and its properties.</p> <p>Parameters: - <code>metric_name</code> (str): Name of the metric to retrieve - <code>task</code> (str, optional): Task type ('classification' or 'regression') for validation</p> <p>Returns: - Tuple containing (metric function, metric name, whether to maximize)</p> <p>Example:</p> <pre><code>metric_func, name, maximize = get_metric('f1', task='classification')\nscore = metric_func(y_true, y_pred)\n</code></pre>"},{"location":"autodask/repository/#get_default_metrictask","title":"<code>get_default_metric(task)</code>","text":"<p>Retrieves the default metric for a specific task type.</p> <p>Parameters: - <code>task</code> (str): Task type ('classification' or 'regression')</p> <p>Returns: - Tuple containing (metric function, metric name, whether to maximize)</p> <p>Example:</p> <pre><code>metric_func, name, maximize = get_default_metric('regression')\nscore = metric_func(y_true, y_pred)\n</code></pre>"},{"location":"autodask/repository/#configuration-files","title":"Configuration Files","text":"<p>The library uses two JSON configuration files:</p> <ol> <li>search_parameters.json - Contains hyperparameter search spaces for each model</li> <li>default_parameters.json - Contains default hyperparameters for each model</li> </ol> <p>Both files should be located in the same directory as the main module.</p>"}]}