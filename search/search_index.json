{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AutoDask","text":"<p>Note: AutoDask is currently in the development stage, which is why some modules may not work or may have errors.</p>"},{"location":"#the-distributed-automl","title":"The Distributed AutoML","text":"<p>AutoDask is a lightweight AutoML library that brings together the power of distributed computing with Dask and the intelligence of Bee Colony Optimization for hyperparameter tuning.</p> <pre><code>pip install autodask  &lt;unsupported yet&gt;\n</code></pre> <pre><code>from autodask.main import AutoDask\n\n# Create an AutoDask instance\nadsk = AutoDask(task='classification')\n\n# Train the model\nadsk.fit(X_train, y_train)\n\n# Make predictions\npredictions = adsk.predict(X_test)\n</code></pre>"},{"location":"#core-features","title":"Core Features","text":"<ul> <li>Multi-Task Support: Classification and regression workflows</li> <li>Distributed Computing: Parallel model training and evaluation</li> <li>Automated Feature Engineering: Intelligent preprocessing and transformation</li> <li>Hyperparameter Optimization: Nature-inspired Bee Colony Optimization algorithm</li> <li>Model Ensembling: Combines top-performing models by using weighted average blending</li> </ul>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Basic Usage</li> <li>API Reference</li> <li>Bee Colony Optimization Details</li> </ul>"},{"location":"#example-use-cases","title":"Example Use Cases","text":"<p>Coming soon...</p>"},{"location":"autodask/advanced_usage/","title":"Using Custom BCO Parameters","text":"<pre><code>from autodask.main import AutoDask\n\n# Configure with custom BCO parameters\nadsk = AutoDask(\n    task='classification',\n    with_tuning=True,\n    bco_params={\n        'employed_bees': 20,     # Number of employed bees\n        'onlooker_bees': 10,     # Number of onlooker bees\n        'scout_bees': 5,         # Number of scout bees\n        'abandonment_limit': 10, # Limit before abandoning a solution\n        'exploration_rate': 0.3  # Balance between exploration and exploitation\n    }\n)\n\n# Train the model\nadsk.fit(X_train, y_train)\n</code></pre>"},{"location":"autodask/basic_usage/","title":"Basic Classification","text":"<pre><code>from autodask.main import AutoDask\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\n\n# Load sample data\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = iris.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create and train AutoDask instance\nadsk = AutoDask(task='classification')\nadsk.fit(X_train, y_train)\n\n# Make predictions\npredictions = adsk.predict(X_test)\n</code></pre>"},{"location":"autodask/basic_usage/#saving-a-model","title":"Saving a Model","text":"<pre><code>from autodask.main import AutoDask\n\n# Create a new instance\nadsk = AutoDask(task='classification')\n\n# Load a saved model\nadsk.save('adsk_model.pkl')\n</code></pre>"},{"location":"autodask/basic_usage/#loading-a-saved-model","title":"Loading a Saved Model","text":"<pre><code>from autodask.main import AutoDask\n\n# Create a new instance\nadsk = AutoDask(task='classification')\n\n# Load a saved model\nadsk.load_model('adsk_model.pkl')\n\n# Use the loaded model to make predictions\npredictions = adsk.predict(X_new)\n</code></pre>"},{"location":"autodask/blender/","title":"WeightedAverageBlender Module","text":""},{"location":"autodask/blender/#overview","title":"Overview","text":"<p>The <code>WeightedAverageBlender</code> class provides an ensemble method that creates weighted combinations of multiple trained models. This blender optimizes the weights for each model to maximize performance on the given task.</p>"},{"location":"autodask/blender/#class-weightedaverageblender","title":"Class: WeightedAverageBlender","text":"<pre><code>from autodask.core.blender import WeightedAverageBlender\n</code></pre>"},{"location":"autodask/blender/#description","title":"Description","text":"<p>A weighted average ensemble blender that combines predictions from multiple pre-trained models by optimizing their weights to maximize performance. The class supports both classification and regression tasks.</p>"},{"location":"autodask/blender/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>fitted_models</code> list required List of dictionaries containing trained models. Each dictionary should have 'model' (fitted estimator) and 'name' (string identifier) keys. <code>task</code> str required The machine learning task type. Supported values: 'classification', 'regression'. <code>max_iter</code> int 100 Maximum number of iterations for weight optimization. <code>tol</code> float 1e-6 Tolerance for convergence in weight optimization. <code>n_classes</code> int None Number of classes for classification tasks. Required for classification, ignored for regression."},{"location":"autodask/blender/#attributes","title":"Attributes","text":"Attribute Type Description <code>weights</code> np.ndarray Optimized weights for each model after fitting. Weights sum to 1.0. <code>fitted_models</code> list List of dictionaries containing the trained models and their names. <code>task</code> str The machine learning task type ('classification' or 'regression'). <code>log</code> Logger Logger instance for tracking progress and optimization details. <code>score_func</code> callable Default scoring function used for weight optimization. <code>metric_name</code> str Name of the metric being optimized. <code>is_maximize_metric</code> bool Whether the metric should be maximized (True) or minimized (False)."},{"location":"autodask/blender/#methods","title":"Methods","text":""},{"location":"autodask/blender/#fitx-y","title":"<code>fit(X, y)</code>","text":"<p>Optimize the weights for ensemble models based on training data.</p> <p>Parameters:</p> Parameter Type Description <code>X</code> Union[pd.DataFrame, np.ndarray] Training features used for weight optimization. <code>y</code> Union[pd.Series, np.ndarray] Training targets used for weight optimization. <p>Returns:</p> <p>The instance itself (self), allowing for method chaining.</p> <p>Example:</p> <pre><code># Assuming you have pre-fitted models\nmodels = [\n    {'model': fitted_lgbm, 'name': 'LGBM'},\n    {'model': fitted_xgb, 'name': 'XGBoost'},\n    {'model': fitted_rf, 'name': 'RandomForest'}\n]\n\n# Create and fit blender\nblender = WeightedAverageBlender(\n    fitted_models=models,\n    task='classification',\n    n_classes=3,\n    max_iter=200\n)\nblender.fit(X_train, y_train)\n</code></pre>"},{"location":"autodask/blender/#predictx","title":"<code>predict(X)</code>","text":"<p>Make predictions using the weighted ensemble.</p> <p>Parameters:</p> Parameter Type Description <code>X</code> Union[pd.DataFrame, np.ndarray] Input features to predict on. <p>Returns:</p> <p>Array of predictions. For classification, returns class labels. For regression, returns continuous values.</p> <p>Example:</p> <pre><code>predictions = blender.predict(X_test)\n</code></pre>"},{"location":"autodask/blender/#predict_probax","title":"<code>predict_proba(X)</code>","text":"<p>For classification tasks, returns the class probabilities for each sample.</p> <p>Parameters:</p> Parameter Type Description <code>X</code> Union[pd.DataFrame, np.ndarray] Input features to predict on. <p>Returns:</p> <p>Array of prediction probabilities for each class. Only available for classification tasks.</p> <p>Example:</p> <pre><code># Only works for classification tasks\nprobabilities = blender.predict_proba(X_test)\n</code></pre>"},{"location":"autodask/blender/#fit_predictx-y","title":"<code>fit_predict(X, y)</code>","text":"<p>Fit the blender and immediately make predictions on the same data.</p> <p>Parameters:</p> Parameter Type Description <code>X</code> Union[pd.DataFrame, np.ndarray] Training features. <code>y</code> Union[pd.Series, np.ndarray] Training targets. <p>Returns:</p> <p>Array of predictions on the training data.</p> <p>Example:</p> <pre><code>train_predictions = blender.fit_predict(X_train, y_train)\n</code></pre>"},{"location":"autodask/blender/#fit_predict_probax-y","title":"<code>fit_predict_proba(X, y)</code>","text":"<p>Fit the blender and immediately return class probabilities on the same data.</p> <p>Parameters:</p> Parameter Type Description <code>X</code> Union[pd.DataFrame, np.ndarray] Training features. <code>y</code> Union[pd.Series, np.ndarray] Training targets. <p>Returns:</p> <p>Array of prediction probabilities for the training data. Only available for classification tasks.</p> <p>Example:</p> <pre><code>train_probabilities = blender.fit_predict_proba(X_train, y_train)\n</code></pre>"},{"location":"autodask/blender/#private-methods","title":"Private Methods","text":"<p>These methods are used internally by the WeightedAverageBlender class:</p> <ul> <li><code>_fit(X, y)</code>: Core optimization logic for finding optimal weights.</li> <li><code>_blend_predictions(predictions, weights, return_labels)</code>: Combines predictions using weighted average.</li> <li><code>_get_model_predictions(X)</code>: Extracts predictions from all ensemble models.</li> <li><code>_validate_before_blending()</code>: Validates that all models are fitted and parameters are correct.</li> </ul>"},{"location":"autodask/blender/#usage-examples","title":"Usage Examples","text":""},{"location":"autodask/blender/#basic-classification-ensemble","title":"Basic Classification Ensemble","text":"<pre><code>from autodask.core.blender import WeightedAverageBlender\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_iris\n\n# Load sample data\niris = load_iris()\nX = pd.DataFrame(iris.data, columns=iris.feature_names)\ny = iris.target\n\n# Split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train individual models\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nlr_model = LogisticRegression(random_state=42)\nxgb_model = XGBClassifier(random_state=42)\n\nrf_model.fit(X_train, y_train)\nlr_model.fit(X_train, y_train)\nxgb_model.fit(X_train, y_train)\n\n# Create model list for blender\nmodels = [\n    {'model': rf_model, 'name': 'RandomForest'},\n    {'model': lr_model, 'name': 'LogisticRegression'},\n    {'model': xgb_model, 'name': 'XGBoost'}\n]\n\n# Create and fit blender\nblender = WeightedAverageBlender(\n    fitted_models=models,\n    task='classification',\n    n_classes=3,\n    max_iter=150\n)\n\n# Fit blender to optimize weights\nblender.fit(X_train, y_train)\n\n# Make predictions\npredictions = blender.predict(X_test)\nprobabilities = blender.predict_proba(X_test)\n\nprint(f\"Optimized weights: {blender.weights}\")\n</code></pre>"},{"location":"autodask/blender/#model-dictionary-format","title":"Model Dictionary Format","text":"<p>The <code>fitted_models</code> parameter expects a list of dictionaries with the following structure:</p> <pre><code>models = [\n    {\n        'model': fitted_sklearn_compatible_model,  # Must be already fitted\n        'name': 'descriptive_model_name'         # String identifier\n    },\n    # ... more models\n]\n</code></pre>"},{"location":"autodask/blender/#notes","title":"Notes","text":"<ul> <li>All models in the ensemble must be pre-fitted before passing to the blender.</li> <li>Single-model ensembles are handled gracefully with a weight of 1.0.</li> <li>Each model must be already fitted (trained)</li> </ul>"},{"location":"autodask/main/","title":"Orchestrating class for AutoDask","text":""},{"location":"autodask/main/#class-autodask","title":"Class: AutoDask","text":"<pre><code>from autodask.main import AutoDask\n</code></pre>"},{"location":"autodask/main/#description","title":"Description","text":"<p>An end-to-end automated machine learning solution that handles preprocessing, model selection, hyperparameter tuning, and ensemble construction. It supports both classification and regression tasks with parallel execution using Dask.</p>"},{"location":"autodask/main/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>task</code> str required The machine learning task type. Supported values: 'classification', 'regression'. <code>n_jobs</code> int 4 Number of parallel jobs to run. Set to -1 to use all available cores. <code>with_tuning</code> bool False Whether to perform hyperparameter tuning using the Bee Colony Optimization algorithm. <code>time_limit</code> int 300 Maximum time in seconds for the AutoML process (default: 5 minutes). <code>metric</code> str None Evaluation metric to optimize. If None, defaults to a task-appropriate metric (accuracy for classification, MSE for regression). <code>cv_folds</code> int 5 Number of cross-validation folds. <code>seed</code> int 101 Random seed for reproducibility. <code>optimization_rounds</code> int 30 Number of optimization rounds for hyperparameter tuning. <code>max_ensemble_models</code> int 3 Maximum number of models in the final ensemble. <code>preprocess</code> bool True Whether to apply automatic preprocessing to the input data. <code>models</code> list None Custom list of models to consider. If None, uses the default model library. <code>bco_params</code> dict {} Parameters for bee colony optimization."},{"location":"autodask/main/#attributes","title":"Attributes","text":"Attribute Type Description <code>ensemble</code> WeightedAverageBlender The final ensemble model after fitting. <code>n_classes</code> int Number of classes (for classification tasks). <code>preprocessor</code> Preprocessor Fitted preprocessing pipeline. <code>log</code> Logger Logger instance for tracking progress."},{"location":"autodask/main/#methods","title":"Methods","text":""},{"location":"autodask/main/#fitx_train-y_train-validation_datanone","title":"<code>fit(X_train, y_train, validation_data=None)</code>","text":"<p>Train the AutoDask model on the given training data.</p> <p>Parameters:</p> Parameter Type Default Description <code>X_train</code> Union[pd.DataFrame, np.ndarray, tuple, dict, list] required Training features. Can be a pandas DataFrame, numpy array, or a tuple/dict/list of arrays (for multi-input models). <code>y_train</code> Union[pd.DataFrame, np.ndarray, tuple, dict, list, str] required Training target. Can be a pandas DataFrame/Series, numpy array, or column name (str) if X_train is a DataFrame. <p>Returns:</p> <p>The instance itself (self), allowing for method chaining.</p> <p>Example:</p> <pre><code># Without validation data (uses k-fold cross-validation)\nadsk = AutoDask(task='classification')\nadsk.fit(X_train, y_train)\n</code></pre>"},{"location":"autodask/main/#predictx_test","title":"<code>predict(X_test)</code>","text":"<p>Make predictions on new data using the trained ensemble.</p> <p>Parameters:</p> Parameter Type Description <code>X_test</code> Union[pd.DataFrame, np.ndarray, tuple, dict, list] Input features to predict on. Should have the same format as X_train in fit(). <p>Returns:</p> <p>Array of predictions. For classification, returns class labels. For regression, returns continuous values.</p> <p>Example:</p> <pre><code>predictions = adsk.predict(X_test)\n</code></pre>"},{"location":"autodask/main/#predict_probax_test","title":"<code>predict_proba(X_test)</code>","text":"<p>For classification tasks, returns the class probabilities for each sample.</p> <p>Parameters:</p> Parameter Type Description <code>X_test</code> Union[pd.DataFrame, np.ndarray, tuple, dict, list] Input features to predict on. Should have the same format as X_train in fit(). <p>Returns:</p> <p>Array of prediction probabilities for each class.</p> <p>Example:</p> <pre><code>probabilities = adsk.predict_proba(X_test)\n</code></pre>"},{"location":"autodask/main/#get_fitted_ensemble","title":"<code>get_fitted_ensemble()</code>","text":"<p>Get the trained ensemble model.</p> <p>Returns:</p> <p>The fitted <code>WeightedAverageBlender</code> ensemble model.</p> <p>Example:</p> <pre><code>ensemble = adsk.get_fitted_ensemble()\n</code></pre>"},{"location":"autodask/main/#get_fitted_preprocessor","title":"<code>get_fitted_preprocessor()</code>","text":"<p>Get the fitted preprocessing pipeline.</p> <p>Returns:</p> <p>The fitted <code>Preprocessor</code> preprocessing pipeline.</p> <p>Example:</p> <pre><code>preprocessor = adsk.get_fitted_preprocessor()\n</code></pre>"},{"location":"autodask/main/#savepath","title":"<code>save(path)</code>","text":"<p>Save the trained ensemble to disk.</p> <p>Parameters:</p> Parameter Type Description <code>path</code> str File path to save the model to. <p>Example:</p> <pre><code>adsk.save('adsk_model.pkl')\n</code></pre>"},{"location":"autodask/main/#load_modelpath","title":"<code>load_model(path)</code>","text":"<p>Load a previously saved ensemble from disk.</p> <p>Parameters:</p> Parameter Type Description <code>path</code> str File path to load the model from. <p>Example:</p> <pre><code>adsk.load_model('adsk_model.pkl')\nadsk.predict(X_new)  # Can now make predictions\n</code></pre>"},{"location":"autodask/main/#private-methods","title":"Private Methods","text":"<p>These methods are used internally by the AutoDask class:</p> <ul> <li><code>_create_dask_server()</code>: Initializes the Dask distributed computing environment.</li> <li><code>_shutdown_dask_server()</code>: Closes the Dask client and cluster.</li> <li><code>_check_input_correctness(X, y)</code>: Validates and formats the input data.</li> <li><code>_kind_clf(y_train)</code>: Determines the classification type (binary or multiclass) and sets <code>n_classes</code>.</li> </ul>"},{"location":"autodask/main/#notes","title":"Notes","text":"<ul> <li>The <code>AutoDask</code> class automatically handles distributed computing setup using Dask, with a local dashboard available at http://localhost:8787/status during training.</li> <li>When <code>preprocess=True</code>, input data is automatically preprocessed, handling categorical features, missing values, and scaling.</li> <li>For classification tasks, the class automatically determines if it's binary or multiclass.</li> <li>The <code>time_limit</code> parameter ensures the AutoML process completes within the specified time constraint.</li> <li>The Bee Colony Optimization algorithm is used for hyperparameter tuning when <code>with_tuning=True</code>.</li> </ul>"},{"location":"autodask/repository/","title":"ML Models and Metrics Repository","text":""},{"location":"autodask/repository/#overview","title":"Overview","text":"<p>AutoDask provides standardized access to machine learning models and evaluation metrics with the following key components:</p> <ol> <li>AtomizedModel class - Repository of machine learning models with default hyperparameters and hyperparameter search spaces</li> <li>Metrics Repository - Collection of standardized evaluation metrics for classification and regression tasks</li> </ol>"},{"location":"autodask/repository/#atomizedmodel-class","title":"AtomizedModel Class","text":"<p>The <code>AtomizedModel</code> class serves as a repository of machine learning models, providing consistent interfaces for accessing various classification and regression algorithms along with their hyperparameter configurations.</p>"},{"location":"autodask/repository/#key-features","title":"Key Features","text":"<ul> <li>Access to standardized machine learning models</li> <li>Default hyperparameters for quick experimentation</li> <li>Hyperparameter search spaces for optimization</li> </ul>"},{"location":"autodask/repository/#available-models","title":"Available Models","text":""},{"location":"autodask/repository/#classification-models","title":"Classification Models","text":"Model Name Implementation Description <code>l2_logreg</code> <code>LogisticRegression</code> Logistic regression with L2 regularization <code>extra_trees</code> <code>ExtraTreesClassifier</code> Ensemble of extremely randomized trees <code>lgbm</code> <code>LGBMClassifier</code> LightGBM gradient boosting classifier <code>xgboost</code> <code>XGBClassifier</code> XGBoost gradient boosting classifier <code>catboost</code> <code>CatBoostClassifier</code> CatBoost gradient boosting classifier"},{"location":"autodask/repository/#regression-models","title":"Regression Models","text":"Model Name Implementation Description <code>l2_linreg</code> <code>LinearRegression</code> Linear regression with L2 regularization <code>extra_trees</code> <code>ExtraTreesRegressor</code> Ensemble of extremely randomized trees <code>lgbm</code> <code>LGBMRegressor</code> LightGBM gradient boosting regressor <code>xgboost</code> <code>XGBRegressor</code> XGBoost gradient boosting regressor <code>catboost</code> <code>CatBoostRegressor</code> CatBoost gradient boosting regressor"},{"location":"autodask/repository/#methods","title":"Methods","text":""},{"location":"autodask/repository/#get_classifier_models","title":"<code>get_classifier_models()</code>","text":"<p>Returns a dictionary of classification models with their hyperparameter spaces and default configurations.</p> <p>Returns: - Dictionary with model names as keys and tuples of (model class, hyperparameter search space, default hyperparameters) as values</p> <p>Example: <pre><code>models = AtomizedModel.get_classifier_models()\nclassifier = models['lgbm'][0](**models['lgbm'][2])  # Create LGBM with default params\n</code></pre></p>"},{"location":"autodask/repository/#get_regressor_models","title":"<code>get_regressor_models()</code>","text":"<p>Returns a dictionary of regression models with their hyperparameter spaces and default configurations.</p> <p>Returns: - Dictionary with model names as keys and tuples of (model class, hyperparameter search space, default hyperparameters) as values</p> <p>Example: <pre><code>models = AtomizedModel.get_regressor_models()\nregressor = models['xgboost'][0](**models['xgboost'][2])  # Create XGBoost with default params\n</code></pre></p>"},{"location":"autodask/repository/#_load_parameter_spaces-private","title":"<code>_load_parameter_spaces()</code> (private)","text":"<p>Loads hyperparameter search spaces from a JSON configuration file.</p> <p>Returns: - Dictionary of parameter search spaces organized by model name</p>"},{"location":"autodask/repository/#_load_parameter_default-private","title":"<code>_load_parameter_default()</code> (private)","text":"<p>Loads default hyperparameters from a JSON configuration file.</p> <p>Returns: - Dictionary of default parameters organized by model name</p>"},{"location":"autodask/repository/#metrics-repository","title":"Metrics Repository","text":"<p>The metrics repository provides a standardized way to access evaluation metrics for machine learning models.</p>"},{"location":"autodask/repository/#available-metrics","title":"Available Metrics","text":""},{"location":"autodask/repository/#classification-metrics","title":"Classification Metrics","text":"Metric Name Function Default Maximize <code>accuracy</code> <code>metrics.accuracy_score</code> Yes Yes <code>f1</code> <code>metrics.f1_score</code> No Yes <code>precision</code> <code>metrics.precision_score</code> No Yes <code>recall</code> <code>metrics.recall_score</code> No Yes"},{"location":"autodask/repository/#regression-metrics","title":"Regression Metrics","text":"Metric Name Function Default Maximize <code>mse</code> <code>metrics.mean_squared_error</code> Yes No <code>r2</code> <code>metrics.r2_score</code> No Yes <code>mae</code> <code>metrics.mean_absolute_error</code> No No <code>rmse</code> Custom (sqrt of MSE) No No"},{"location":"autodask/repository/#functions","title":"Functions","text":""},{"location":"autodask/repository/#get_metricmetric_name-tasknone","title":"<code>get_metric(metric_name, task=None)</code>","text":"<p>Retrieves a specific metric function and its properties.</p> <p>Parameters: - <code>metric_name</code> (str): Name of the metric to retrieve - <code>task</code> (str, optional): Task type ('classification' or 'regression') for validation</p> <p>Returns: - Tuple containing (metric function, metric name, whether to maximize)</p> <p>Example: <pre><code>metric_func, name, maximize = get_metric('f1', task='classification')\nscore = metric_func(y_true, y_pred)\n</code></pre></p>"},{"location":"autodask/repository/#get_default_metrictask","title":"<code>get_default_metric(task)</code>","text":"<p>Retrieves the default metric for a specific task type.</p> <p>Parameters: - <code>task</code> (str): Task type ('classification' or 'regression')</p> <p>Returns: - Tuple containing (metric function, metric name, whether to maximize)</p> <p>Example: <pre><code>metric_func, name, maximize = get_default_metric('regression')\nscore = metric_func(y_true, y_pred)\n</code></pre></p>"},{"location":"autodask/repository/#configuration-files","title":"Configuration Files","text":"<p>The library uses two JSON configuration files:</p> <ol> <li>search_parameters.json - Contains hyperparameter search spaces for each model</li> <li>default_parameters.json - Contains default hyperparameters for each model</li> </ol> <p>Both files should be located in the same directory as the main module.</p>"},{"location":"autodask/trainer/","title":"Trainer","text":"<p>Coming soon...</p>"},{"location":"autodask/tuner/","title":"BeeColonyOptimizer Module","text":""},{"location":"autodask/tuner/#overview","title":"Overview","text":"<p>The <code>BeeColonyOptimizer</code> class provides an simplified implementation of the Bee Colony Optimization (BCO) algorithm for hyperparameter tuning in machine learning models. This algorithm is inspired by the foraging behavior of honey bees and is effective for exploring complex hyperparameter spaces to find optimal configurations.</p>"},{"location":"autodask/tuner/#class-beecolonyoptimizer","title":"Class: BeeColonyOptimizer","text":"<pre><code>from autodask.core.optimizer import BeeColonyOptimizer\n</code></pre>"},{"location":"autodask/tuner/#description","title":"Description","text":"<p>A hyperparameter optimization solution based on the Bee Colony Optimization algorithm. It efficiently explores the hyperparameter space to find optimal model configurations for both classification and regression tasks.</p>"},{"location":"autodask/tuner/#constructor-parameters","title":"Constructor Parameters","text":"Parameter Type Default Description <code>task</code> str required The machine learning task type. Supported values: 'classification', 'regression'. <code>employed_bees</code> int 3 Number of employed bees in the colony. These bees explore the neighborhood of existing solutions. <code>onlooker_bees</code> int 3 Number of onlooker bees in the colony. These bees focus on promising solutions. <code>exploration_rate</code> float 0.3 Balance between exploration (random search) and exploitation (neighborhood refinement). Higher values favor exploration. <code>cv_folds</code> int 2 Number of cross-validation folds for evaluating solutions."},{"location":"autodask/tuner/#attributes","title":"Attributes","text":"Attribute Type Description <code>solutions</code> list Current solutions maintained by the colony (each solution is a tuple of parameters and score). <code>solution_trials</code> dict Tracks how many times each solution has been tried without improvement. <code>task</code> str The machine learning task type ('classification' or 'regression'). <code>cv_folds</code> int Number of cross-validation folds. <code>log</code> Logger Logger instance for tracking progress."},{"location":"autodask/tuner/#methods","title":"Methods","text":""},{"location":"autodask/tuner/#optimizemodel_class-param_space-x_train-y_train-metric_func-maximizetrue-rounds1-time_limitnone","title":"<code>optimize(model_class, param_space, X_train, y_train, metric_func, maximize=True, rounds=1, time_limit=None)</code>","text":"<p>Run the bee colony optimization algorithm to find optimal hyperparameters.</p> <p>Parameters:</p> Parameter Type Default Description <code>model_class</code> Class required The machine learning model class to optimize. <code>param_space</code> dict required Parameter search space defining possible values for each hyperparameter. <code>X_train</code> Union[pd.DataFrame, np.ndarray] required Training features. <code>y_train</code> Union[pd.Series, np.ndarray] required Training target. <code>metric_func</code> callable required Scoring function to evaluate model performance. <code>maximize</code> bool True Whether to maximize or minimize the metric. Set to True for metrics like accuracy, F1; False for metrics like error, loss. <code>rounds</code> int 1 Number of optimization rounds to perform. <code>time_limit</code> int None Maximum time in seconds for optimization. If None, no time limit is applied. <p>Returns:</p> <p>A tuple containing the best parameters found and their corresponding score.</p>"},{"location":"autodask/tuner/#initialize_colonymodel_class-param_space-x_train-y_train-metric_func","title":"<code>initialize_colony(model_class, param_space, X_train, y_train, metric_func)</code>","text":"<p>Initialize bee colony with random solutions.</p> <p>Parameters:</p> Parameter Type Description <code>model_class</code> Class The machine learning model class to optimize. <code>param_space</code> dict Parameter search space defining possible values for each hyperparameter. <code>X_train</code> Union[pd.DataFrame, np.ndarray] Training features. <code>y_train</code> Union[pd.Series, np.ndarray] Training target. <code>metric_func</code> callable Scoring function to evaluate model performance. <p>Example:</p> <pre><code>optimizer.initialize_colony(\n    model_class=LGBMClassifier,\n    param_space=param_space,\n    X_train=X_train,\n    y_train=y_train,\n    metric_func=accuracy_score\n)\n</code></pre>"},{"location":"autodask/tuner/#update_best_solutionbest_params-best_score-maximize","title":"<code>update_best_solution(best_params, best_score, maximize)</code>","text":"<p>Updates the best solution found by the colony.</p> <p>Parameters:</p> Parameter Type Description <code>best_params</code> dict Current best parameters. <code>best_score</code> float Current best score. <code>maximize</code> bool Whether to maximize or minimize the score. <p>Returns:</p> <p>A tuple containing the updated best parameters and best score.</p>"},{"location":"autodask/tuner/#employed_bees_phasemodel_class-param_space-x_train-y_train-metric_func-maximize","title":"<code>employed_bees_phase(model_class, param_space, X_train, y_train, metric_func, maximize)</code>","text":"<p>Executes the employed bees phase where bees explore the neighborhood of existing solutions.</p> <p>Parameters:</p> Parameter Type Description <code>model_class</code> Class The machine learning model class to optimize. <code>param_space</code> dict Parameter search space defining possible values for each hyperparameter. <code>X_train</code> Union[pd.DataFrame, np.ndarray] Training features. <code>y_train</code> Union[pd.Series, np.ndarray] Training target. <code>metric_func</code> callable Scoring function to evaluate model performance. <code>maximize</code> bool Whether to maximize or minimize the score."},{"location":"autodask/tuner/#onlooker_bees_phasemodel_class-param_space-x_train-y_train-metric_func-maximize","title":"<code>onlooker_bees_phase(model_class, param_space, X_train, y_train, metric_func, maximize)</code>","text":"<p>Executes the onlooker bees phase where bees focus on promising solutions.</p> <p>Parameters:</p> Parameter Type Description <code>model_class</code> Class The machine learning model class to optimize. <code>param_space</code> dict Parameter search space defining possible values for each hyperparameter. <code>X_train</code> Union[pd.DataFrame, np.ndarray] Training features. <code>y_train</code> Union[pd.Series, np.ndarray] Training target. <code>metric_func</code> callable Scoring function to evaluate model performance. <code>maximize</code> bool Whether to maximize or minimize the score."},{"location":"autodask/tuner/#private-methods","title":"Private Methods","text":"<p>These methods are used internally by the BeeColonyOptimizer class:</p> <ul> <li><code>_calculate_selection_probabilities(maximize)</code>: Calculates probability distribution for solution selection by onlooker bees.</li> <li><code>_get_params_key(params)</code>: Creates a stable hashable key for parameter dictionaries.</li> <li><code>_generate_random_params(param_space)</code>: Generates random parameters from the parameter space.</li> <li><code>_explore_neighborhood(params, param_space)</code>: Generates a new solution by exploring neighborhood of current solution.</li> </ul>"},{"location":"autodask/tuner/#usage-examples","title":"Usage Examples","text":""},{"location":"autodask/tuner/#basic-hyperparameter-optimization-for-classification","title":"Basic Hyperparameter Optimization for Classification","text":"<pre><code>from sklearn.metrics import accuracy_score as accuracy\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom autodask.core.tuner import BeeColonyOptimizer\n\n\nxt_params_space = {\n    \"n_estimators\": [50, 100, 200, 300],\n    \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n    \"max_depth\": [None, 5, 10, 20, 30],\n    \"min_samples_split\": [2, 5, 10],\n    \"min_samples_leaf\": [1, 2, 4],\n    \"max_features\": [\"sqrt\", \"log2\", None],\n    \"bootstrap\": [False, True]\n}\n\nbco = BeeColonyOptimizer(task='classification')\n\nbest_params, best_score = bco.optimize(\n    model_class=ExtraTreesClassifier,\n    param_space=xt_params_space,\n    X_train=X_train,\n    y_train=y_train,\n    metric_func=accuracy,\n    maximize=True,\n    rounds=1,\n    time_limit=None\n)\n\nprint(f'Best score: {best_score}')\nprint(f'Best parameters: \\n {best_params}')\n\n# Now you can use optimized parameters...\n</code></pre>"},{"location":"autodask/tuner/#notes","title":"Notes","text":"<ul> <li>The <code>BeeColonyOptimizer</code> uses cross-validation to evaluate each solution, making it robust to overfitting.</li> <li>A large number of rounds can take a very long time to process.</li> </ul>"}]}